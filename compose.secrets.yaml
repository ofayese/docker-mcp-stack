# Docker MCP Stack - Docker Compose Configuration with Docker Secrets
# Complete setup for running multiple AI models with MCP integration using Docker Secrets for sensitive data

version: "3.9"

# Docker Secrets Configuration
secrets:
  github_token:
    external: true
  gitlab_token:
    external: true
  postgres_password:
    external: true
  grafana_admin_password:
    external: true
  backup_encryption_key:
    external: true

services:
  #
  # MODEL RUNNERS
  # All models are exposed via OpenAI-compatible API endpoints
  #
  
  # SmolLM2 Model Runner
  smollm2-runner:
    image: ai/smollm2
    container_name: smollm2-runner
    ports:
      - "${SMOLLM2_PORT:-12434}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
      - basic
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Llama3 Model Runner
  llama3-runner:
    image: ai/llama3
    container_name: llama3-runner
    ports:
      - "${LLAMA3_PORT:-12435}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Qwen3 Model Runner
  qwen3-runner:
    image: ai/qwen3
    container_name: qwen3-runner
    ports:
      - "${QWEN3_PORT:-12437}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Qwen2.5 Model Runner
  qwen2-runner:
    image: ai/qwen2.5
    container_name: qwen2-runner
    ports:
      - "${QWEN2_PORT:-12438}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Mistral Model Runner
  mistral-runner:
    image: ai/mistral
    container_name: mistral-runner
    ports:
      - "${MISTRAL_PORT:-12439}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Gemma3 Model Runner
  gemma3-runner:
    image: ai/gemma3
    container_name: gemma3-runner
    ports:
      - "${GEMMA3_PORT:-12440}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Granite 7B Lab Model Runner
  granite7-runner:
    image: redhat/granite-7b-lab-gguf
    container_name: granite7-runner
    ports:
      - "${GRANITE7_PORT:-12441}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Granite 3 8B Instruct Model Runner
  granite3-runner:
    image: ai/granite-3-8b-instruct
    container_name: granite3-runner
    ports:
      - "${GRANITE3_PORT:-12442}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  #
  # MCP SERVERS
  # Model Context Protocol servers providing various capabilities
  #

  # MCP Filesystem Server - Provides file system access
  mcp-filesystem:
    image: mcp/filesystem
    container_name: mcp-filesystem
    command:
      - /rootfs
    volumes:
      - fs_data:/rootfs
    restart: unless-stopped
    profiles:
      - basic
      - mcp

  # MCP Postgres Server - Provides PostgreSQL database access
  mcp-postgres:
    image: mcp/postgres
    container_name: mcp-postgres-server
    environment:
      # Using Docker Secrets for secure database connection
      POSTGRES_CONNECTION_STRING_FILE: /run/secrets/postgres_connection_string
      # Security: Disable superuser creation
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    profiles:
      - basic
      - mcp
    secrets:
      - source: postgres_password
        target: postgres_connection_string
        mode: 0400

  # MCP Git Server - Provides Git operations
  mcp-git:
    image: mcp/git
    container_name: mcp-git
    volumes:
      - git_data:/workspace
    restart: unless-stopped
    profiles:
      - basic
      - mcp

  # MCP SQLite Server - Provides SQLite database access
  mcp-sqlite:
    image: mcp/sqlite
    container_name: mcp-sqlite
    volumes:
      - sqlite_data:/data
    restart: unless-stopped
    profiles:
      - basic
      - mcp

  # MCP GitHub Server - Provides GitHub API access with Docker Secrets
  mcp-github:
    image: mcp/github
    container_name: mcp-github
    environment:
      # Docker Secrets will mount the token as a file
      GITHUB_PERSONAL_ACCESS_TOKEN_FILE: /run/secrets/github_token
    restart: unless-stopped
    profiles:
      - github
      - mcp
    secrets:
      - source: github_token
        target: github_token
        mode: 0400

  # MCP GitLab Server - Provides GitLab API access with Docker Secrets
  mcp-gitlab:
    image: mcp/gitlab
    container_name: mcp-gitlab
    environment:
      # Docker Secrets will mount the token as a file
      GITLAB_PERSONAL_ACCESS_TOKEN_FILE: /run/secrets/gitlab_token
    restart: unless-stopped
    profiles:
      - gitlab
      - mcp
    secrets:
      - source: gitlab_token
        target: gitlab_token
        mode: 0400

  # MCP Sentry Server - Provides Sentry monitoring integration
  mcp-sentry:
    image: mcp/sentry
    container_name: mcp-sentry
    environment:
      SENTRY_DSN: "${SENTRY_DSN:-your-sentry-dsn}"
    restart: unless-stopped
    profiles:
      - sentry
      - mcp

  # MCP Everything Server - Combines capabilities with Docker Secrets
  mcp-everything:
    image: mcp/everything
    container_name: mcp-everything
    environment:
      # Using Docker Secrets for secure access
      POSTGRES_CONNECTION_STRING_FILE: /run/secrets/postgres_connection_string
      GITHUB_PERSONAL_ACCESS_TOKEN_FILE: /run/secrets/github_token
      GITLAB_PERSONAL_ACCESS_TOKEN_FILE: /run/secrets/gitlab_token
      SENTRY_DSN: "${SENTRY_DSN:-your-sentry-dsn}"
    volumes:
      - fs_data:/rootfs
      - git_data:/workspace
      - sqlite_data:/data
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    profiles:
      - everything
      - mcp
    secrets:
      - source: postgres_password
        target: postgres_connection_string
        mode: 0400
      - source: github_token
        target: github_token
        mode: 0400
      - source: gitlab_token
        target: gitlab_token
        mode: 0400

  #
  # INFRASTRUCTURE SERVICES
  # Core services required for the stack to function
  #

  # PostgreSQL Database - Using Docker Secrets for password
  postgres:
    image: postgres:15-alpine
    container_name: mcp-postgres
    environment:
      POSTGRES_DB: "${POSTGRES_DB:-mcp}"
      POSTGRES_USER: "${POSTGRES_USER:-mcp}"
      # Using Docker Secrets for password
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      # Security: Disable superuser creation
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init:/docker-entrypoint-initdb.d:ro
    restart: unless-stopped
    profiles:
      - basic
      - infrastructure
    # Security: Run as non-root user
    user: "999:999"
    # Security: Resource limits
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: '1'
        reservations:
          memory: 512m
          cpus: '0.5'
    # Security: Read-only root filesystem where possible
    read_only: true
    tmpfs:
      - /tmp
      - /var/run/postgresql
    # Security: Drop unnecessary capabilities
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-mcp} -d ${POSTGRES_DB:-mcp}"]
      interval: 10s
      timeout: 5s
      retries: 5
    secrets:
      - source: postgres_password
        target: postgres_password
        mode: 0400

  # Nginx - Reverse proxy for all model runners
  nginx:
    image: nginx:alpine
    container_name: mcp-nginx
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/html:/usr/share/nginx/html:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    restart: unless-stopped
    depends_on:
      - smollm2-runner
    profiles:
      - basic
      - infrastructure
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Prometheus - Monitoring and metrics
  prometheus:
    image: prom/prometheus:latest
    container_name: mcp-prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    profiles:
      - monitoring
      - infrastructure
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana - Visualization and dashboards with Docker Secrets
  grafana:
    image: grafana/grafana:latest
    container_name: mcp-grafana
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      # Using Docker Secrets for admin password
      - GF_SECURITY_ADMIN_PASSWORD__FILE=/run/secrets/grafana_admin_password
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    restart: unless-stopped
    depends_on:
      - prometheus
    profiles:
      - monitoring
      - infrastructure
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    secrets:
      - source: grafana_admin_password
        target: grafana_admin_password
        mode: 0400

  # GPU Monitoring (if available)
  nvidia-smi-exporter:
    image: mindprince/nvidia_gpu_prometheus_exporter:0.1
    container_name: mcp-gpu-exporter
    ports:
      - "${GPU_EXPORTER_PORT:-9445}:9445"
    restart: unless-stopped
    profiles:
      - monitoring
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  model_cache:
    driver: local
  fs_data:
    driver: local
  git_data:
    driver: local
  sqlite_data:
    driver: local
  postgres_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  default:
    name: mcp-stack
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

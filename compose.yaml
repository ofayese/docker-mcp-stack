# Docker MCP Stack - Docker Compose Configuration
# Complete setup for running multiple AI models with MCP integration

version: "3.9"

services:
  #
  # MODEL RUNNERS
  # All models are exposed via OpenAI-compatible API endpoints
  #
  
  # SmolLM2 Model Runner
  smollm2-runner:
    image: ai/smollm2
    container_name: smollm2-runner
    ports:
      - "${SMOLLM2_PORT:-12434}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
      - basic
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Llama3 Model Runner
  llama3-runner:
    image: ai/llama3
    container_name: llama3-runner
    ports:
      - "${LLAMA3_PORT:-12435}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Phi-4 Model Runner
  phi4-runner:
    image: ai/phi4
    container_name: phi4-runner
    ports:
      - "${PHI4_PORT:-12436}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Qwen3 Model Runner
  qwen3-runner:
    image: ai/qwen3
    container_name: qwen3-runner
    ports:
      - "${QWEN3_PORT:-12437}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Qwen2.5 Model Runner
  qwen2-runner:
    image: ai/qwen2.5
    container_name: qwen2-runner
    ports:
      - "${QWEN2_PORT:-12438}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Mistral Model Runner
  mistral-runner:
    image: ai/mistral
    container_name: mistral-runner
    ports:
      - "${MISTRAL_PORT:-12439}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Gemma3 Model Runner
  gemma3-runner:
    image: ai/gemma3
    container_name: gemma3-runner
    ports:
      - "${GEMMA3_PORT:-12440}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Granite 7B Lab Model Runner
  granite7-runner:
    image: redhat/granite-7b-lab-gguf
    container_name: granite7-runner
    ports:
      - "${GRANITE7_PORT:-12441}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Granite 3 8B Instruct Model Runner
  granite3-runner:
    image: ai/granite-3-8b-instruct
    container_name: granite3-runner
    ports:
      - "${GRANITE3_PORT:-12442}:12434"
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_DEVICES:-0}
    volumes:
      - model_cache:/app/models
    restart: unless-stopped
    profiles:
      - models
    deploy:
      resources:
        limits:
          memory: ${MODEL_MEMORY_LIMIT:-4g}
          cpus: '${MODEL_CPU_LIMIT:-2}'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12434/engines/v1/models"]
      interval: ${HEALTH_CHECK_INTERVAL:-30s}
      timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
      retries: ${HEALTH_CHECK_RETRIES:-3}
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  #
  # MCP SERVERS
  # These servers provide additional capabilities to Gordon AI assistant
  #

  # MCP Time Server - Provides temporal capabilities
  mcp-time:
    image: mcp/time
    container_name: mcp-time
    restart: unless-stopped
    profiles:
      - basic
      - mcp

  # MCP Fetch Server - Provides web fetch capabilities
  mcp-fetch:
    image: mcp/fetch
    container_name: mcp-fetch
    restart: unless-stopped
    profiles:
      - basic
      - mcp

  # MCP Filesystem Server - Provides filesystem access
  mcp-filesystem:
    image: mcp/filesystem
    container_name: mcp-filesystem
    command:
      - /rootfs
    volumes:
      - fs_data:/rootfs
    restart: unless-stopped
    profiles:
      - basic
      - mcp

  # MCP Postgres Server - Provides PostgreSQL database access
  mcp-postgres:
    image: mcp/postgres
    container_name: mcp-postgres-server
    environment:
      POSTGRES_CONNECTION_STRING: "postgresql://${POSTGRES_USER:-mcp}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-mcp}"
      # Security: Disable superuser creation
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    profiles:
      - basic
      - mcp
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "nc", "-z", "postgres", "5432"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MCP Git Server - Provides Git operations
  mcp-git:
    image: mcp/git
    container_name: mcp-git
    volumes:
      - git_data:/workspace
    restart: unless-stopped
    profiles:
      - basic
      - mcp
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "git", "--version"]
      interval: 60s
      timeout: 10s
      retries: 3

  # MCP SQLite Server - Provides SQLite database access
  mcp-sqlite:
    image: mcp/sqlite
    container_name: mcp-sqlite
    volumes:
      - sqlite_data:/data
    restart: unless-stopped
    profiles:
      - basic
      - mcp
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "sqlite3", "/data/test.db", ".tables"]
      interval: 60s
      timeout: 10s
      retries: 3

  # MCP GitHub Server - Provides GitHub API access
  mcp-github:
    image: mcp/github
    container_name: mcp-github
    environment:
      GITHUB_PERSONAL_ACCESS_TOKEN: "${GITHUB_TOKEN}"
    restart: unless-stopped
    profiles:
      - github
      - mcp
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "curl", "-f", "https://api.github.com/rate_limit"]
      interval: 60s
      timeout: 10s
      retries: 3

  # MCP GitLab Server - Provides GitLab API access
  mcp-gitlab:
    image: mcp/gitlab
    container_name: mcp-gitlab
    environment:
      GITLAB_PERSONAL_ACCESS_TOKEN: "${GITLAB_TOKEN}"
    restart: unless-stopped
    profiles:
      - gitlab
      - mcp
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "curl", "-f", "https://gitlab.com/api/v4/user"]
      interval: 60s
      timeout: 10s
      retries: 3

  # MCP Sentry Server - Provides Sentry monitoring integration
  mcp-sentry:
    image: mcp/sentry
    container_name: mcp-sentry
    environment:
      SENTRY_DSN: "${SENTRY_DSN}"
    restart: unless-stopped
    profiles:
      - sentry
      - mcp
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "curl", "-f", "https://sentry.io/api/0/"]
      interval: 60s
      timeout: 10s
      retries: 3

  # MCP Everything Server - Combines capabilities of various MCP servers
  mcp-everything:
    image: mcp/everything
    container_name: mcp-everything
    environment:
      POSTGRES_CONNECTION_STRING: "postgresql://${POSTGRES_USER:-mcp}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-mcp}"
      GITHUB_PERSONAL_ACCESS_TOKEN: "${GITHUB_TOKEN}"
      GITLAB_PERSONAL_ACCESS_TOKEN: "${GITLAB_TOKEN}"
      SENTRY_DSN: "${SENTRY_DSN}"
    volumes:
      - fs_data:/rootfs
      - git_data:/workspace
      - sqlite_data:/data
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    profiles:
      - everything
      - mcp
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: '1'

  #
  # INFRASTRUCTURE SERVICES
  # Core services required for the stack to function
  #

  # PostgreSQL Database - Used by MCP Postgres Server
  postgres:
    image: postgres:15-alpine
    container_name: mcp-postgres
    environment:
      POSTGRES_DB: "${POSTGRES_DB:-mcp}"
      POSTGRES_USER: "${POSTGRES_USER:-mcp}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      # Security: Disable superuser creation
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init:/docker-entrypoint-initdb.d:ro
    restart: unless-stopped
    profiles:
      - basic
      - infrastructure
    # Security: Run as non-root user
    user: "999:999"
    # Security: Resource limits
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: '1'
        reservations:
          memory: 512m
          cpus: '0.5'
    # Security: Read-only root filesystem where possible
    read_only: true
    tmpfs:
      - /tmp
      - /var/run/postgresql
    # Security: Drop unnecessary capabilities
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-mcp} -d ${POSTGRES_DB:-mcp}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Nginx - Reverse proxy for all model runners
  nginx:
    image: nginx:alpine
    container_name: mcp-nginx
    ports:
      - "${NGINX_HTTP_PORT:-80}:80"
      - "${NGINX_HTTPS_PORT:-443}:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/html:/usr/share/nginx/html:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    restart: unless-stopped
    depends_on:
      - smollm2-runner
    profiles:
      - web
      - infrastructure
    # Security: Run as non-root user
    user: "101:101"
    # Security: Resource limits
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: '0.5'
        reservations:
          memory: 128m
          cpus: '0.25'
    # Security: Read-only root filesystem
    read_only: true
    tmpfs:
      - /var/cache/nginx
      - /var/run
      - /tmp
    # Security: Drop unnecessary capabilities
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - NET_BIND_SERVICE
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-3}

  #
  # MONITORING SERVICES
  # Services for monitoring the stack
  #

  # Prometheus - Metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: mcp-prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION:-15d}'
      - '--web.enable-admin-api'
    restart: unless-stopped
    profiles:
      - monitoring
    # Security: Run as non-root user
    user: "65534:65534"
    # Security: Resource limits
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: '1'
        reservations:
          memory: 512m
          cpus: '0.5'
    # Security: Read-only root filesystem where possible
    read_only: true
    tmpfs:
      - /tmp
    # Security: Drop unnecessary capabilities
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-3}

  # Grafana - Visualization
  grafana:
    image: grafana/grafana:latest
    container_name: mcp-grafana
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SECURITY_DISABLE_GRAVATAR=true
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_STRICT_TRANSPORT_SECURITY=true
      - GF_LOG_LEVEL=${GRAFANA_LOG_LEVEL:-info}
    restart: unless-stopped
    depends_on:
      - prometheus
    profiles:
      - monitoring
    # Security: Run as non-root user
    user: "472:472"
    # Security: Resource limits
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: '0.5'
        reservations:
          memory: 256m
          cpus: '0.25'
    # Security: Drop unnecessary capabilities
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-3}

  # Node Exporter - System metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: mcp-node-exporter
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
      - '--web.disable-exporter-metrics'
      - '--collector.disable-defaults'
      - '--collector.cpu'
      - '--collector.diskstats'
      - '--collector.filesystem'
      - '--collector.loadavg'
      - '--collector.meminfo'
      - '--collector.netdev'
      - '--collector.time'
      - '--collector.uname'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    restart: unless-stopped
    profiles:
      - monitoring
    # Security: Run as non-root user
    user: "65534:65534"
    # Security: Resource limits
    deploy:
      resources:
        limits:
          memory: 128m
          cpus: '0.25'
        reservations:
          memory: 64m
          cpus: '0.1'
    # Security: Read-only root filesystem
    read_only: true
    # Security: Drop unnecessary capabilities
    cap_drop:
      - ALL
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "${LOG_MAX_SIZE:-10m}"
        max-file: "${LOG_MAX_FILES:-3}"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9100/metrics"]
      interval: ${HEALTHCHECK_INTERVAL:-30s}
      timeout: ${HEALTHCHECK_TIMEOUT:-10s}
      retries: ${HEALTHCHECK_RETRIES:-3}

volumes:
  # Model data
  model_cache:
    name: mcp_model_cache
    driver: local

  # MCP server data
  fs_data:
    name: mcp_fs_data
    driver: local
  git_data:
    name: mcp_git_data
    driver: local
  sqlite_data:
    name: mcp_sqlite_data
    driver: local
  postgres_data:
    name: mcp_postgres_data
    driver: local

  # Monitoring data
  prometheus_data:
    name: mcp_prometheus_data
    driver: local
  grafana_data:
    name: mcp_grafana_data
    driver: local

networks:
  default:
    name: mcp_network
    driver: bridge
